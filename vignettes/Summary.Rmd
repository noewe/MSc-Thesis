---
title: "Summary"
author: 'Moritz Burger & Noémie Wellinger'
date: "`r Sys.Date()`"
output: html_document
---

This Rmd script calculates important Urban Climate indicators for each logger. These are first calculated for each day of the measurement period, and finally summarized (averages) in a big summary table. Note that the time of day over they are calculated differs: whole day (00-24), daytime (6-22) and night (either 22-6 or 21-7).

\*\* NOT SURE HOW EXACTLY WE SHOULD PROCEED FOR THE VALID DAYS. THIS IS NOÉMIE'S SUGGESTION: **To contextualize the number of hot day for example, we also check the amount of valid days, as in no more than 20% NA. If there are more than 20% NA, all the data of this day is deleted.** 20% OF COURSE COULD ALSO BE 10%. ALSO, DOES IT MAKE SENSE FOR ALL INDICATORS TO USE THE SAME THRESHOLD? \*\*

```{r libraries, include=F}
library(tidyverse)
library(dplyr)
library(lubridate)
library(plotly)
library(weathermetrics)
```

# Preprocessing

```{r}
# Set variables
year <- 2024
# date_start = "2024-06-01"
# date_end = "2024-08-31"
date_start = "2024-05-01"
date_end = "2024-10-30"
date_start = as.Date(date_start, tz = "Europe/Zurich")
date_end = as.Date(date_end, tz = "Europe/Zurich")

city = "Thun"

variable = "T"
#variable = "H"

quality = "raw"
#quality = "QC"

ndays = as.numeric(difftime(as.Date(date_end)+1, as.Date(date_start), units = "days"))
ndays
```

```{r}
# Define Data and Metadata
data_T <- read_csv("../data/Thun Messnetz 2024/Logger_data_T_2024-05-01_2024-10-30.csv")
data_H <- read_csv("../data/Thun Messnetz 2024/Logger_data_H_2024-05-01_2024-10-30.csv")

meta <- read_csv('../data/Metadata/metadata_Thun.csv', show_col_types = FALSE)
meta_LU <- read_csv('../data/Metadata/Land_Use_Thun.csv', show_col_types = FALSE)

meta <- meta |>
  dplyr::select(Log_NR, STANDORT, NORD_CHTOPO, OST_CHTOPO, LV_03_N, LV_03_E) |>
  mutate(Log_NR = paste0("Log_", Log_NR)) |>
  left_join(meta_LU, by = c("STANDORT" = "Location")) |>
  rename(Log_NR = Log_NR.x) |>
  distinct() #remove duplicate rows
```

```{r}
# Check the time column
# if the first column of data is not called "time", change the name of the first column to "time"
if(colnames(data_T)[1] != "time"){
  colnames(data_T)[1] <- "time"
}

# Check the timezone of the time column
timezone <- attr(data_T$time, "tzone")
print(paste("Timezone is ", timezone))
```

Convert timezone if needed

```{r}
# get a list of Olson timezones
OlsonNames()
```

```{r}
# Set the current timezone, target timezone and time format
timezone <- "UTC" 
target_timezone <- "Europe/Zurich" 
format <- "%Y-%m-%d %H:%M"  # or %d.%m.%Y %H:%M or %Y-%m-%d %H:%M:%S or %d/%m/%Y %H:%M

# Convert:
data_T$time <- as.POSIXct(data_T$time, format = format, tz = timezone)
data_H$time <- as.POSIXct(data_H$time, format = format, tz = timezone)
# Shift to the target timezone
data_T$time <- with_tz(data_T$time, tzone = target_timezone)
data_H$time <- with_tz(data_H$time, tzone = target_timezone)
print(paste("Timezone is now set to", attr(data_T$time, "tzone"), "for data_T and", attr(data_H$time, "tzone"), "for data_H."))
```

```{r read, include = F}
# Shift the hours, so that the night is not cut in half
# cut the data to beginning and end date
data_T <- data_T |>
  filter(time >= date_start, time <= date_end) |>   # cut from date_start to date_end
  mutate(timeshift = time - dhours(7))  # 07 is the new 00
data_H <- data_H |>
  filter(time >= date_start, time <= date_end) |>
  mutate(timeshift = time - dhours(7))

visdat::vis_miss(
  data_T, #your table
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```

# Summarize the data

## Long data format
```{r}
data_T_long <- data_T |>
  pivot_longer(cols = starts_with("Log_"), names_to = "Log_NR", values_to = "Temperature")

data_H_long <- data_H |>
  pivot_longer(cols = starts_with("Log_"), names_to = "Log_NR", values_to = "Humidity")

data_long <- data_T_long |>
  full_join(data_H_long, by = c("time", "timeshift", "Log_NR")) |>
  dplyr::select(time, timeshift, Log_NR, Temperature, Humidity)

# Calculate indices
data_long <- data_long |>
  # UHI
  left_join(# Create a reference table with Log_401's temperature at each time
    data_long |>
      filter(Log_NR == "Log_401")|>
      select(time, Temperature_401 = Temperature), 
    by = "time"
  ) |>
  mutate(UHI = Temperature - Temperature_401) |>
  select(-Temperature_401) |>
  # Heat Index
  mutate(Heat_Index = heat.index(t = Temperature, rh = Humidity, temperature.metric = "celsius", output.metric = "celsius", round = 2)) 

#calculate cooling rate
data_long <- data_long |>
  group_by(Log_NR) |>
  mutate(change_rate = c(NA, diff(x = Temperature, k = 1)))

write_csv(data_long, (paste0("../data/Thun Messnetz 2024/10min_long_", date_start, "_", date_end, ".csv")))
```

## Aggregate the data

### Hourly
```{r}
## Aggregate hourly, this may take long (like 2mins)

# According to MeteoCH, the aggregation should be like this:
# HH = (HH-1):01 to HH:00
# E.g., 13 includes 12:01 to 13:00

data_long_hourly <- data_long |>
  #mutate(time_hour = floor_date(time, unit = "hour")) |> # round to nearest hour (floor)
  mutate(time_hour = floor_date(time - minutes(1), unit = "hour") + hours(1)) |>
  group_by(time_hour, Log_NR) |>
  summarise(
    T_mean = mean(Temperature, na.rm = TRUE),
    T_min = min(Temperature, na.rm = TRUE),
    T_max = max(Temperature, na.rm = TRUE),
    H_mean = mean(Humidity, na.rm = TRUE),
    H_min = min(Humidity, na.rm = TRUE),
    H_max = max(Humidity, na.rm = TRUE),
    UHI_mean = mean(UHI, na.rm = TRUE),
    UHI_min = min(UHI, na.rm = TRUE),
    UHI_max = max(UHI, na.rm = TRUE),
    HI_mean = mean(Heat_Index, na.rm = TRUE),
    HI_min = min(Heat_Index, na.rm = TRUE),
    HI_max = max(Heat_Index, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(across(everything(), ~ ifelse(is.nan(.) | is.infinite(.), NA, .))) |>
  mutate(time_hour = as.POSIXct(time_hour, origin = "1970-01-01", tz = target_timezone)) # Re-convert to POSIXct object

# recalculate mean UHI
data_long_hourly <- data_long_hourly |>
  # Re-alculate UHI from hourly means
  left_join(# Create a reference table with Log_401's temperature at each time
    data_long_hourly |>
      filter(Log_NR == "Log_401")|>
      select(time_hour, Temperature_401 = T_mean), 
    by = c("time_hour")
  ) |>
  mutate(UHI_mean = T_mean - Temperature_401) |>
  select(-Temperature_401)
  # Re-Calculate Heat Index
  #mutate(Heat_Index_hourly = heat.index(t = Temperature, rh = Humidity, temperature.metric = "celsius", output.metric = "celsius", round = 2)) 
  # Checkt if there is a difference between the hourly and the daily calculation - there is one
  #mutate(UHI_diff = round(UHI - UHI_hourly, 2), Heat_Index_diff = round(Heat_Index - Heat_Index_hourly, 2))

#calculate cooling rate
data_long_hourly <- data_long_hourly |>
  group_by(Log_NR) |>
  mutate(change_rate = c(NA, diff(x = T_mean, k = 1)))

#save as csv and excel
write_csv(data_long_hourly, (paste0("../data/Thun Messnetz 2024/Thun_hourly_summary_", date_start, "_", date_end, ".csv")))
```

### Diurnal
```{r}
data_long_diurnal_MJJASO <- data_long_hourly |>
  mutate(hour=hour(time_hour)) |>
  filter(month(time_hour) %in% c(5:10)) |>
  group_by(hour, Log_NR) |>
  summarise(across(c(T_mean, H_mean, UHI_mean, HI_mean), ~ mean(., na.rm = TRUE)),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)))

data_long_diurnal_JAS <- data_long_hourly |>
  mutate(hour=hour(time_hour)) |>
  filter(month(time_hour) %in% c(7:9)) |>
  group_by(hour, Log_NR) |>
  summarise(across(c(T_mean, H_mean, UHI_mean, HI_mean), ~ mean(., na.rm = TRUE)),
            # add 95th upper and lower percentile
            T_mean_95 = quantile(T_mean, 0.95, na.rm = TRUE),
            T_mean_5 = quantile(T_mean, 0.05, na.rm = TRUE),
            UHI_mean_95 = quantile(UHI_mean, 0.95, na.rm = TRUE),
            UHI_mean_5 = quantile(UHI_mean, 0.05, na.rm = TRUE),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)))

write_csv(data_long_diurnal_MJJASO, (paste0("../data/Thun Messnetz 2024/Thun_diurnal_MJJASO.csv")))
write_csv(data_long_diurnal_JAS, (paste0("../data/Thun Messnetz 2024/Thun_diurnal_JAS.csv")))
```

### Daily
```{r}
## Aggregate daily
### 1. Data aggregated from hourly data: Mean values. This compensates for missing 10-min data points if some hours have more NA values than others.
data_long_daily <- data_long_hourly |>
  mutate(time_day = date(time_hour - hours(1))) |> # 01:00 (current day) to 00:00 (next day) is aggregated
  group_by(time_day, Log_NR) |>
  summarise(valid_hours = sum(!is.na(T_mean)), # Calculate valid hours as number of hours per day that contain data (not NA)
            across(c(T_mean, H_mean, UHI_mean, HI_mean), ~ mean(., na.rm = TRUE)),
            across(c(T_min, H_min, UHI_min, HI_min), ~ min(., na.rm = TRUE)),
            across(c(T_max, H_max, UHI_max, HI_max), ~ max(., na.rm = TRUE)),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .))) |>
  mutate(time_day = as.POSIXct(time_day, origin = "2024-05-01", format = "%Y-%m-%d", tz = target_timezone)) # Re-convert to POSIXct object


### 2. Re-calculate UHI from daily means
data_long_daily2 <- data_long_daily |>
  # re-calculate UHI
  left_join(# Create a reference table with Log_401's temperature at each time
    data_long_daily |>
      filter(Log_NR == "Log_401")|>
      select(time_day, Temperature_401 = T_mean), 
    by = c("time_day")
  ) |>
  mutate(UHI_mean = T_mean - Temperature_401) |>
  select(-Temperature_401) 


### 3. Nighttime values and indicators (21:00 - 07:00) following day: Tmin, Tropical Nights, Gfrörli Nights
data_long_daily_night1 <- data_long_hourly |>
  mutate(timeshift = time_hour - dhours(7),
         time_day = floor_date(time_hour, unit = "day")) |> # 07 is the new 00
  filter(hour(timeshift) >= 14) |>
  group_by(time_day, Log_NR) |>
  summarise(valid_hours_night = sum(!is.na(T_min)),
            T_min_night = min(T_min, na.rm = TRUE),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)),
         time_day = as.POSIXct(time_day, origin = "1970-01-01", tz = target_timezone)) |> # Re-convert to POSIXct object
  # Add indicators
  mutate(Tropical_night = ifelse(T_min_night >= 20 & T_min_night < 30, 1, 0),
         Gfroerli_night = ifelse(T_min_night <= 10 & T_min_night >= 0, 1, 0)) 


### 4. Nighttime values and indicators (22:00 - 06:00 following day): T_mean, nighttime UHI
data_long_daily_night2 <- data_long_hourly |>
  mutate(timeshift = time_hour - dhours(6),
         time_day = floor_date(time_hour, unit = "day")) |> # 07 is the new 00
  filter(hour(timeshift) >= 16) |>
  group_by(time_day, Log_NR) |>
  summarise(T_mean_night = mean(T_mean, na.rm = TRUE),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)),
         time_day = as.POSIXct(time_day, origin = "1970-01-01", tz = target_timezone)) # Re-convert to POSIXct object

# Re-calculate UHI from T_mean_night
data_long_daily_night2 <- data_long_daily_night2 |>
  left_join(# Create a reference table with Log_401's temperature at each time
    data_long_daily_night2 |>
      filter(Log_NR == "Log_401")|>
      select(time_day, Temperature_401 = T_mean_night), 
    by = c("time_day")
  ) |>
  mutate(UHI_night = T_mean_night - Temperature_401) |>
  select(-Temperature_401)

# Join night data
data_long_daily_night <- data_long_daily_night1 |>
  full_join(data_long_daily_night2, by = c("time_day", "Log_NR")) |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)),
         time_day = as.POSIXct(time_day, origin = "1970-01-01", tz = target_timezone))


### 5. Daytime values and indicators (06:00 - 22:00): T_mean, daytime UHI
data_long_daily_day <- data_long_hourly |>
  filter(hour(time_hour) >= 6, hour(time_hour) < 22) |>
  mutate(time_day = floor_date(time_hour, unit = "day")) |> # 07 is the new 00
  group_by(time_day, Log_NR) |>
  summarise(valid_hours_day = sum(!is.na(T_mean)),
            T_mean_day = mean(T_mean, na.rm = TRUE),
            T_max_day = max(T_max, na.rm = TRUE),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)),
         time_day = as.POSIXct(time_day, origin = "1970-01-01", tz = target_timezone)) |> # Re-convert to POSIXct object
  # Add daytime indicators
  mutate(Summer_day = ifelse(T_max_day >= 25 & T_max_day < 30, 1, 0),
         Hot_day = ifelse(T_max_day >= 30 & T_max_day < 35, 1, 0),
         Very_hot_day = ifelse(T_max_day >= 35, 1, 0))

# Re-calculate UHI from T_mean_day
data_long_daily_day <- data_long_daily_day |>
  left_join(# Create a reference table with Log_401's temperature at each time
    data_long_daily_day |>
      filter(Log_NR == "Log_401")|>
      select(time_day, Temperature_401 = T_mean_day), 
    by = c("time_day")
  ) |>
  mutate(UHI_day = T_mean_day - Temperature_401) |>
  select(-Temperature_401)

# 6. Delete data if not enough valid hours
data_long_daily2 <- data_long_daily2 |>
  filter(valid_hours >= 19) # Remove days with less than 80% valid hours (24h max)

data_long_daily_night <- data_long_daily_night |>
  filter(valid_hours_night >= 8) # Remove days with less than 80% valid hours (10h max)

data_long_daily_day <- data_long_daily_day |>
  filter(valid_hours_day >= 12) # Remove days with less than 80% valid hours (16h max)


# 7. Combine all daily data
data_long_daily <- data_long_daily2 |>
  full_join(data_long_daily_night, by = c("time_day", "Log_NR")) |>
  full_join(data_long_daily_day, by = c("time_day", "Log_NR")) |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)),
         time_day = as.POSIXct(time_day, origin = "1970-01-01")) |>
  # remove hours and minutes from time_day
  mutate(time_day = as.POSIXct(format(time_day, "%Y-%m-%d"))) 

# Delete unused dataframes
rm(data_long_daily2, data_long_daily_night, data_long_daily_day, data_long_daily_night1, data_long_daily_night2, data_long_daily_day)


write_csv(data_long_daily, (paste0("../data/Thun Messnetz 2024/Thun_daily_summary_", date_start, "_", date_end, ".csv")))
```

### Monthly

```{r}
# Aggregate monthly
data_long_monthly <- data_long_daily |>
  group_by(month = month(time_day), Log_NR) |>
  summarise(across(c(T_mean, T_mean_day, T_mean_night, H_mean, UHI_mean, UHI_day, UHI_night, HI_mean), ~ mean(., na.rm = TRUE)),
            across(c(T_min, T_min_night, H_min, UHI_min, HI_min), ~ min(., na.rm = TRUE)),
            across(c(T_max, T_max_day, H_max, UHI_max, HI_max), ~ max(., na.rm = TRUE)),
            across(c(Tropical_night, Gfroerli_night, Summer_day, Hot_day, Very_hot_day), ~ sum(., na.rm = TRUE)),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)))

write_csv(data_long_monthly, (paste0("../data/Thun Messnetz 2024/Thun_monthly_summary_", date_start, "_", date_end, ".csv")))
```

### Season
```{r}
# Aggregate over whole time period
data_long_MJJASO <- data_long_daily |>
  filter(month(time_day) %in% c(5:10)) |>
  group_by(Log_NR) |>
  summarise(across(c(T_mean, T_mean_day, T_mean_night, H_mean, UHI_mean, UHI_day, UHI_night, HI_mean), ~ mean(., na.rm = TRUE)),
            across(c(T_min, T_min_night, H_min, UHI_min, HI_min), ~ min(., na.rm = TRUE)),
            across(c(T_max, T_max_day, H_max, UHI_max, HI_max), ~ max(., na.rm = TRUE)),
            across(c(Tropical_night, Gfroerli_night, Summer_day, Hot_day, Very_hot_day), ~ sum(., na.rm = TRUE)),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)))

data_long_JAS <- data_long_daily |>
  filter(month(time_day) %in% c(7:9)) |>
  group_by(Log_NR) |>
  summarise(across(c(T_mean, T_mean_day, T_mean_night, H_mean, UHI_mean, UHI_day, UHI_night, HI_mean), ~ mean(., na.rm = TRUE)),
            across(c(T_min, T_min_night, H_min, UHI_min, HI_min), ~ min(., na.rm = TRUE)),
            across(c(T_max, T_max_day, H_max, UHI_max, HI_max), ~ max(., na.rm = TRUE)),
            across(c(Tropical_night, Gfroerli_night, Summer_day, Hot_day, Very_hot_day), ~ sum(., na.rm = TRUE)),
            .groups = "drop") |>
  mutate(across(everything(), ~ ifelse(is.nan(.)|is.infinite(.), NA, .)))

write_csv(data_long_MJJASO, (paste0("../data/Thun Messnetz 2024/Thun_MJJASO_summary_2024.csv")))
write_csv(data_long_JAS, (paste0("../data/Thun Messnetz 2024/Thun_JAS_summary_2024.csv")))
```







Add the coordinates from the metadata and format them correctly. Important for mapping later on.

```{r coordinates, include = F}
Summary <- Summary |>
  full_join(meta, by = 'Log_name') |>
  drop_na() |>
  distinct() #Remove identical rows
```

